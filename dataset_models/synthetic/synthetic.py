import numpy as np
import random
import dataset_models.dataset

class Synthetic(dataset_models.dataset.Dataset):
    """A synthetic data generator. This class is meant to eliminate
   uncertainty in the images experienced by the neural network to test
   and validate models. You should use this data generator to
   prototype networks, because if your network cannot model data
   generated by this class, then it will not be able able to model the
   data generated by the actual sun. The model includes parameters for
   the following elements:

    * Pixel dynamic range, and dependent variable dynamic range: The
      sun has an extreme amount of variability that can confuse the
      gradient.  By setting the dynamic range, you can check whether
      the training is robust to this.

    * Pixel scale, and dependent variable scale: Check for numerical
      and other issues that may confuse training.

    * Noise layer count: The number of layers in the image that are
      just noise.

    * Signal layer count: The number of layers in the image whose sum
      produces the dependent variable.

    * Independent variable scalar: The scalar applied to the sum of
      the signal layers that determines the dependent variable.

    * Dependent variable noise: How much additive noise to introduce
      to the dependent variable.

    * Dimensions: The size of the image.

    The recommended procedure for utilizing this domain is to first
    train on the default parameters, then vary the parameters
    one-at-a-time. Finally, a random search over parameters should
    highlight if your network model will likely fail on the actual
    sun.

    """

    def __init__(self, samples_per_step=32, input_width=2,
                 input_height=2, training_set_size=1000, validation_set_size=1000,
                 scale_signal_pixels=1.0, scale_noise_pixels=1.0,
                 scale_dependent_variable=1.0,
                 dependent_variable_additive_noise_variance=0.0,
                 noise_channel_count=0, signal_channel_count=1,
                 active_regions=False):
        """
        Keyword arguments:
            samples_per_step -- the batch size for the training and validation generators (default 32)
            input_width -- the width of the solar image in pixels (default 2)
            input_height -- the height of the solar image in pixels (default 2)
            training_set_size -- the number of distinct samples in the training set (default 1000)
            validation_set_size -- the number of distinct samples in the validation set (default 1000)
            scale_signal_pixels -- The scale factor that is applied to each of the random signal matrices (default 1.0)
            scale_dependent_variable -- The scale factor applied to the dependent variable (default 1.0)
            dependent_variable_additive_noise_variance -- The variance of the gaussian noise added to the dependent variable (default 0.0)
            noise_channel_count -- The number of layers that do not determine the dependent variable (default 0)
            signal_channel_count -- The number of layers that determine the dependent variable (default 1)
            active_regions -- Should the signal be arranged into active regions? (default False)
        """
        self.samples_per_step = samples_per_step
        self.input_width = input_width
        self.input_height = input_height
        self.training_set_size = training_set_size
        self.validation_set_size = validation_set_size
        self.scale_signal_pixels = scale_signal_pixels
        self.scale_noise_pixels = scale_noise_pixels
        self.scale_dependent_variable = scale_dependent_variable
        self.dependent_variable_additive_noise_variance = dependent_variable_additive_noise_variance
        self.noise_channel_count = noise_channel_count
        self.signal_channel_count = signal_channel_count
        self.active_regions = active_regions
        if active_regions:
            raise NotImplementedError

    def get_dimensions(self):
        return (self.input_width, self.input_height,
                self.noise_channel_count + self.signal_channel_count)

    def get_validation_step_count(self):
        """The number of datapoints to generate for validation"""
        return self.validation_set_size

    def download_dataset(self):
        """This is not a dataset_model defined by data, so this can just
        return.

        """
        return

    def is_downloaded(self):
        """Confirm that the dataset has been successfully downloaded"""
        return True

    def _get_noise_channel(self):
        """Generate a channel that is entirely noise, with no signal.
        """
        channel = np.random.random((self.input_width, self.input_height))
        channel *= self.scale_noise_pixels
        return channel

    def _get_signal_channel(self):
        """Generate a channel whose sum is the dependent variable"""
        channel = np.random.random((self.input_width, self.input_height))
        channel *= self.scale_signal_pixels
        return channel

    def _get_sample(self, index):
        """Get a sample by assigning the random seed to the index and fetching
        the layers.
        """
        np.random.seed(seed=index)
        noise_channels = []
        signal_channels = []
        sample = []
        for _ in range(0, self.signal_channel_count):
            channel = self._get_signal_channel()
            signal_channels.append(channel)
            sample.append(channel)
        for _ in range(0, self.noise_channel_count):
            channel = self._get_noise_channel()
            noise_channels.append(channel)
            sample.append(channel)

        y = 0
        for channel in signal_channels:
            y += np.sum(channel)
        y *= self.scale_dependent_variable

        if self.dependent_variable_additive_noise_variance > 0.0:
            mu, sigma = 0, self.dependent_variable_additive_noise_variance # mean and standard deviation
            s = np.random.normal(mu, sigma, 1)
            y += s[0]
        sample = np.asarray(sample)
        sample = np.rollaxis(sample, 0, 3)
        return (sample, y)

    def get_training_generator(self):
        """Generate training samples
        """
        return self._get_generator(training=True)

    def get_validation_generator(self):
        """Generate validation samples"""
        return self._get_generator(training=False)

    def _get_generator(self, training=True):
        """Implementation of data generator"""
        if training:
            dataset_size = self.training_set_size
        else:
            dataset_size = self.validation_set_size
        data_x = []
        data_y = []
        i = 0
        sample_indices = range(0, dataset_size)
        shuffle_count = 0
        while 1:
            sample = self._get_sample(sample_indices[i])
            data_x_sample = sample[0]
            data_y_sample = sample[1]
            data_x.append(data_x_sample)
            data_y.append(data_y_sample)

            i += 1

            if i == len(sample_indices):
                i = 0
                sample_indices = range(0, dataset_size)
                random.seed(shuffle_count)
                random.shuffle(sample_indices)
                shuffle_count += 1

            if self.samples_per_step == len(data_x):
                ret_x = np.reshape(data_x, (len(data_x), self.input_width, self.input_height, self.noise_channel_count + self.signal_channel_count))
                ret_y = np.reshape(data_y, (len(data_y)))
                yield (ret_x, ret_y)
                data_x = []
                data_y = []
